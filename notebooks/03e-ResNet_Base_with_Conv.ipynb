{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Working Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from importlib.util import find_spec\n",
    "if find_spec(\"core\") is None:\n",
    "    import sys\n",
    "    sys.path.append('..')\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from core.datasets import RetinaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code snippet helps if your computer has RTX 2070 GPU. If not then comment this cell.\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_train_info = tfds.load('RetinaDataset', split='train[:98%]', shuffle_files=True, as_supervised=True,with_info=True)\n",
    "ds_val, ds_val_info     = tfds.load('RetinaDataset', split='train[-2%:]', shuffle_files=True, as_supervised=True,with_info=True)\n",
    "ds_test, ds_test_info   = tfds.load('RetinaDataset', split='test', shuffle_files=True, as_supervised=True,with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimCLR Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title SimCLR DataUtils\n",
    "\n",
    "# coding=utf-8\n",
    "# Copyright 2020 The SimCLR Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific simclr governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Data preprocessing and augmentation.\"\"\"\n",
    "\n",
    "import functools\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "CROP_PROPORTION = 0.875  # Standard for ImageNet.\n",
    "\n",
    "\n",
    "def random_apply(func, p, x):\n",
    "    \"\"\"Randomly apply function func to x with probability p.\"\"\"\n",
    "    return tf.cond(\n",
    "      tf.less(\n",
    "          tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
    "          tf.cast(p, tf.float32)), lambda: func(x), lambda: x)\n",
    "\n",
    "\n",
    "def random_brightness(image, max_delta, impl='simclrv2'):\n",
    "    \"\"\"A multiplicative vs additive change of brightness.\"\"\"\n",
    "    if impl == 'simclrv2':\n",
    "        factor = tf.random.uniform([], tf.maximum(1.0 - max_delta, 0),\n",
    "                                   1.0 + max_delta)\n",
    "        image = image * factor\n",
    "    elif impl == 'simclrv1':\n",
    "        image = tf.image.random_brightness(image, max_delta=max_delta)\n",
    "    else:\n",
    "        raise ValueError('Unknown impl {} for random brightness.'.format(impl))\n",
    "    return image\n",
    "\n",
    "\n",
    "def to_grayscale(image, keep_channels=True):\n",
    "    image = tf.image.rgb_to_grayscale(image)\n",
    "    if keep_channels:\n",
    "        image = tf.tile(image, [1, 1, 3])\n",
    "    return image\n",
    "\n",
    "\n",
    "def color_jitter(image, strength, random_order=True, impl='simclrv2'):\n",
    "    \"\"\"Distorts the color of the image.\n",
    "    Args:\n",
    "    image: The input image tensor.\n",
    "    strength: the floating number for the strength of the color augmentation.\n",
    "    random_order: A bool, specifying whether to randomize the jittering order.\n",
    "    impl: 'simclrv1' or 'simclrv2'.  Whether to use simclrv1 or simclrv2's\n",
    "        version of random brightness.\n",
    "    Returns:\n",
    "    The distorted image tensor.\n",
    "    \"\"\"\n",
    "    brightness = 0.8 * strength\n",
    "    contrast = 0.8 * strength\n",
    "    saturation = 0.8 * strength\n",
    "    hue = 0.2 * strength\n",
    "    if random_order:\n",
    "        return color_jitter_rand(\n",
    "            image, brightness, contrast, saturation, hue, impl=impl)\n",
    "    else:\n",
    "        return color_jitter_nonrand(\n",
    "            image, brightness, contrast, saturation, hue, impl=impl)\n",
    "\n",
    "\n",
    "def color_jitter_nonrand(image,\n",
    "                         brightness=0,\n",
    "                         contrast=0,\n",
    "                         saturation=0,\n",
    "                         hue=0,\n",
    "                         impl='simclrv2'):\n",
    "    \"\"\"Distorts the color of the image (jittering order is fixed).\n",
    "    Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "    impl: 'simclrv1' or 'simclrv2'.  Whether to use simclrv1 or simclrv2's\n",
    "        version of random brightness.\n",
    "    Returns:\n",
    "    The distorted image tensor.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('distort_color'):\n",
    "        def apply_transform(i, x, brightness, contrast, saturation, hue):\n",
    "            \"\"\"Apply the i-th transformation.\"\"\"\n",
    "            if brightness != 0 and i == 0:\n",
    "                x = random_brightness(x, max_delta=brightness, impl=impl)\n",
    "            elif contrast != 0 and i == 1:\n",
    "                x = tf.image.random_contrast(\n",
    "                    x, lower=1-contrast, upper=1+contrast)\n",
    "            elif saturation != 0 and i == 2:\n",
    "                x = tf.image.random_saturation(\n",
    "                    x, lower=1-saturation, upper=1+saturation)\n",
    "            elif hue != 0:\n",
    "                x = tf.image.random_hue(x, max_delta=hue)\n",
    "            return x\n",
    "\n",
    "        for i in range(4):\n",
    "            image = apply_transform(i, image, brightness, contrast, saturation, hue)\n",
    "        return image\n",
    "\n",
    "\n",
    "def color_jitter_rand(image,\n",
    "                      brightness=0,\n",
    "                      contrast=0,\n",
    "                      saturation=0,\n",
    "                      hue=0,\n",
    "                      impl='simclrv2'):\n",
    "    \"\"\"Distorts the color of the image (jittering order is random).\n",
    "    Args:\n",
    "    image: The input image tensor.\n",
    "    brightness: A float, specifying the brightness for color jitter.\n",
    "    contrast: A float, specifying the contrast for color jitter.\n",
    "    saturation: A float, specifying the saturation for color jitter.\n",
    "    hue: A float, specifying the hue for color jitter.\n",
    "    impl: 'simclrv1' or 'simclrv2'.  Whether to use simclrv1 or simclrv2's\n",
    "        version of random brightness.\n",
    "    Returns:\n",
    "    The distorted image tensor.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('distort_color'):\n",
    "        def apply_transform(i, x):\n",
    "            \"\"\"Apply the i-th transformation.\"\"\"\n",
    "            def brightness_foo():\n",
    "                if brightness == 0:\n",
    "                    return x\n",
    "                else:\n",
    "                    return random_brightness(x, max_delta=brightness, impl=impl)\n",
    "\n",
    "            def contrast_foo():\n",
    "                if contrast == 0:\n",
    "                    return x\n",
    "                else:\n",
    "                    return tf.image.random_contrast(x, lower=1-contrast, upper=1+contrast)\n",
    "            def saturation_foo():\n",
    "                if saturation == 0:\n",
    "                    return x\n",
    "                else:\n",
    "                    return tf.image.random_saturation(\n",
    "                      x, lower=1-saturation, upper=1+saturation)\n",
    "            def hue_foo():\n",
    "                if hue == 0:\n",
    "                    return x\n",
    "                else:\n",
    "                    return tf.image.random_hue(x, max_delta=hue)\n",
    "            x = tf.cond(tf.less(i, 2),\n",
    "                  lambda: tf.cond(tf.less(i, 1), brightness_foo, contrast_foo),\n",
    "                  lambda: tf.cond(tf.less(i, 3), saturation_foo, hue_foo))\n",
    "            return x\n",
    "\n",
    "        perm = tf.random.shuffle(tf.range(4))\n",
    "        for i in range(4):\n",
    "            image = apply_transform(perm[i], image)\n",
    "        return image\n",
    "\n",
    "\n",
    "def _compute_crop_shape(\n",
    "    image_height, image_width, aspect_ratio, crop_proportion):\n",
    "    \"\"\"Compute aspect ratio-preserving shape for central crop.\n",
    "    The resulting shape retains `crop_proportion` along one side and a proportion\n",
    "    less than or equal to `crop_proportion` along the other side.\n",
    "    Args:\n",
    "    image_height: Height of image to be cropped.\n",
    "    image_width: Width of image to be cropped.\n",
    "    aspect_ratio: Desired aspect ratio (width / height) of output.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "    Returns:\n",
    "    crop_height: Height of image after cropping.\n",
    "    crop_width: Width of image after cropping.\n",
    "    \"\"\"\n",
    "    image_width_float = tf.cast(image_width, tf.float32)\n",
    "    image_height_float = tf.cast(image_height, tf.float32)\n",
    "\n",
    "    def _requested_aspect_ratio_wider_than_image():\n",
    "        crop_height = tf.cast(\n",
    "            tf.math.rint(crop_proportion / aspect_ratio * image_width_float),\n",
    "            tf.int32)\n",
    "        crop_width = tf.cast(\n",
    "            tf.math.rint(crop_proportion * image_width_float), tf.int32)\n",
    "        return crop_height, crop_width\n",
    "\n",
    "    def _image_wider_than_requested_aspect_ratio():\n",
    "        crop_height = tf.cast(\n",
    "            tf.math.rint(crop_proportion * image_height_float), tf.int32)\n",
    "        crop_width = tf.cast(\n",
    "            tf.math.rint(crop_proportion * aspect_ratio * image_height_float),\n",
    "            tf.int32)\n",
    "        return crop_height, crop_width\n",
    "\n",
    "    return tf.cond(\n",
    "      aspect_ratio > image_width_float / image_height_float,\n",
    "      _requested_aspect_ratio_wider_than_image,\n",
    "      _image_wider_than_requested_aspect_ratio)\n",
    "\n",
    "\n",
    "def center_crop(image, height, width, crop_proportion):\n",
    "    \"\"\"Crops to center of image and rescales to desired size.\n",
    "    Args:\n",
    "    image: Image Tensor to crop.\n",
    "    height: Height of image to be cropped.\n",
    "    width: Width of image to be cropped.\n",
    "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
    "    Returns:\n",
    "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
    "    \"\"\"\n",
    "    shape = tf.shape(image)\n",
    "    image_height = shape[0]\n",
    "    image_width = shape[1]\n",
    "    crop_height, crop_width = _compute_crop_shape(\n",
    "      image_height, image_width, height / width, crop_proportion)\n",
    "    offset_height = ((image_height - crop_height) + 1) // 2\n",
    "    offset_width = ((image_width - crop_width) + 1) // 2\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "      image, offset_height, offset_width, crop_height, crop_width)\n",
    "\n",
    "    image = tf.image.resize([image], [height, width],\n",
    "                          method=tf.image.ResizeMethod.BICUBIC)[0]\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def distorted_bounding_box_crop(image,\n",
    "                                bbox,\n",
    "                                min_object_covered=0.1,\n",
    "                                aspect_ratio_range=(0.75, 1.33),\n",
    "                                area_range=(0.05, 1.0),\n",
    "                                max_attempts=100,\n",
    "                                scope=None):\n",
    "    \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n",
    "    See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
    "    Args:\n",
    "    image: `Tensor` of image data.\n",
    "    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
    "        where each coordinate is [0, 1) and the coordinates are arranged\n",
    "        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
    "        image.\n",
    "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
    "        area of the image must contain at least this fraction of any bounding\n",
    "        box supplied.\n",
    "    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
    "        image must have an aspect ratio = width / height within this range.\n",
    "    area_range: An optional list of `float`s. The cropped area of the image\n",
    "        must contain a fraction of the supplied image within in this range.\n",
    "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
    "        region of the image of the specified constraints. After `max_attempts`\n",
    "        failures, return the entire image.\n",
    "    scope: Optional `str` for name scope.\n",
    "    Returns:\n",
    "    (cropped image `Tensor`, distorted bbox `Tensor`).\n",
    "    \"\"\"\n",
    "    with tf.name_scope(scope or 'distorted_bounding_box_crop'):\n",
    "        shape = tf.shape(image)\n",
    "        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
    "            shape,\n",
    "            bounding_boxes=bbox,\n",
    "            min_object_covered=min_object_covered,\n",
    "            aspect_ratio_range=aspect_ratio_range,\n",
    "            area_range=area_range,\n",
    "            max_attempts=max_attempts,\n",
    "            use_image_if_no_bounding_boxes=True)\n",
    "        bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
    "\n",
    "        # Crop the image to the specified bounding box.\n",
    "        offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
    "        target_height, target_width, _ = tf.unstack(bbox_size)\n",
    "        image = tf.image.crop_to_bounding_box(\n",
    "            image, offset_y, offset_x, target_height, target_width)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "def crop_and_resize(image, height, width):\n",
    "    \"\"\"Make a random crop and resize it to height `height` and width `width`.\n",
    "    Args:\n",
    "    image: Tensor representing the image.\n",
    "    height: Desired image height.\n",
    "    width: Desired image width.\n",
    "    Returns:\n",
    "    A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
    "    \"\"\"\n",
    "    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
    "    aspect_ratio = width / height\n",
    "    image = distorted_bounding_box_crop(\n",
    "      image,\n",
    "      bbox,\n",
    "      min_object_covered=0.1,\n",
    "      aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
    "      area_range=(0.08, 1.0),\n",
    "      max_attempts=100,\n",
    "      scope=None)\n",
    "    return tf.image.resize([image], [height, width],\n",
    "                         method=tf.image.ResizeMethod.BICUBIC)[0]\n",
    "\n",
    "\n",
    "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
    "    \"\"\"Blurs the given image with separable convolution.\n",
    "    Args:\n",
    "    image: Tensor of shape [height, width, channels] and dtype float to blur.\n",
    "    kernel_size: Integer Tensor for the size of the blur kernel. This is should\n",
    "      be an odd number. If it is an even number, the actual kernel size will be\n",
    "      size + 1.\n",
    "    sigma: Sigma value for gaussian operator.\n",
    "    padding: Padding to use for the convolution. Typically 'SAME' or 'VALID'.\n",
    "    Returns:\n",
    "    A Tensor representing the blurred image.\n",
    "    \"\"\"\n",
    "    radius = tf.cast(kernel_size / 2, dtype=tf.int32)\n",
    "    kernel_size = radius * 2 + 1\n",
    "    x = tf.cast(tf.range(-radius, radius + 1), dtype=tf.float32)\n",
    "    blur_filter = tf.exp(-tf.pow(x, 2.0) /\n",
    "                       (2.0 * tf.pow(tf.cast(sigma, dtype=tf.float32), 2.0)))\n",
    "    blur_filter /= tf.reduce_sum(blur_filter)\n",
    "    # One vertical and one horizontal filter.\n",
    "    blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
    "    blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
    "    num_channels = tf.shape(image)[-1]\n",
    "    blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
    "    blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
    "    expand_batch_dim = image.shape.ndims == 3\n",
    "    if expand_batch_dim:\n",
    "        # Tensorflow requires batched input to convolutions, which we can fake with\n",
    "        # an extra dimension.\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "    blurred = tf.nn.depthwise_conv2d(\n",
    "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
    "    blurred = tf.nn.depthwise_conv2d(\n",
    "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
    "    if expand_batch_dim:\n",
    "        blurred = tf.squeeze(blurred, axis=0)\n",
    "    return blurred\n",
    "\n",
    "\n",
    "def random_crop_with_resize(image, height, width, p=1.0):\n",
    "    \"\"\"Randomly crop and resize an image.\n",
    "    Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: Probability of applying this transformation.\n",
    "    Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "    \"\"\"\n",
    "    def _transform(image):  # pylint: disable=missing-docstring\n",
    "        image = crop_and_resize(image, height, width)\n",
    "        return image\n",
    "    return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_color_jitter(image, p=1.0, impl='simclrv2', color_jitter_strength=0.8):\n",
    "\n",
    "    def _transform(image):\n",
    "        color_jitter_t = functools.partial(\n",
    "            color_jitter, strength=color_jitter_strength, impl=impl)\n",
    "        image = random_apply(color_jitter_t, p=0.8, x=image)\n",
    "        return random_apply(to_grayscale, p=0.2, x=image)\n",
    "    return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def random_blur(image, height, width, p=1.0):\n",
    "    \"\"\"Randomly blur an image.\n",
    "    Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    p: probability of applying this transformation.\n",
    "    Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "    \"\"\"\n",
    "    del width\n",
    "    def _transform(image):\n",
    "        sigma = tf.random.uniform([], 0.1, 2.0, dtype=tf.float32)\n",
    "        return gaussian_blur(image, kernel_size=height//10, sigma=sigma, padding='SAME')\n",
    "    return random_apply(_transform, p=p, x=image)\n",
    "\n",
    "\n",
    "def batch_random_blur(images_list, height, width, blur_probability=0.5):\n",
    "    \"\"\"Apply efficient batch data transformations.\n",
    "    Args:\n",
    "    images_list: a list of image tensors.\n",
    "    height: the height of image.\n",
    "    width: the width of image.\n",
    "    blur_probability: the probaility to apply the blur operator.\n",
    "    Returns:\n",
    "    Preprocessed feature list.\n",
    "    \"\"\"\n",
    "    def generate_selector(p, bsz):\n",
    "        shape = [bsz, 1, 1, 1]\n",
    "        selector = tf.cast(\n",
    "            tf.less(tf.random.uniform(shape, 0, 1, dtype=tf.float32), p),\n",
    "            tf.float32)\n",
    "        return selector\n",
    "\n",
    "    new_images_list = []\n",
    "    for images in images_list:\n",
    "        images_new = random_blur(images, height, width, p=1.)\n",
    "        selector = generate_selector(blur_probability, tf.shape(images)[0])\n",
    "        images = images_new * selector + images * (1 - selector)\n",
    "        images = tf.clip_by_value(images, 0., 1.)\n",
    "        new_images_list.append(images)\n",
    "\n",
    "    return new_images_list\n",
    "\n",
    "\n",
    "def preprocess_for_train(image,\n",
    "                         height,\n",
    "                         width,\n",
    "                         color_distort=True,\n",
    "                         crop=True,\n",
    "                         flip=True,\n",
    "                         color_jitter_strength=0.9,\n",
    "                         impl='simclrv2'):\n",
    "    \"\"\"Preprocesses the given image for training.\n",
    "    Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    color_distort: Whether to apply the color distortion.\n",
    "    crop: Whether to crop the image.\n",
    "    flip: Whether or not to flip left and right of an image.\n",
    "    impl: 'simclrv1' or 'simclrv2'.  Whether to use simclrv1 or simclrv2's\n",
    "        version of random brightness.\n",
    "    Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "    \"\"\"\n",
    "    if crop:\n",
    "        image = random_crop_with_resize(image, height, width)\n",
    "    if flip:\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "    if color_distort:\n",
    "        image = random_color_jitter(image, impl=impl, \n",
    "                                    color_jitter_strength=color_jitter_strength)\n",
    "    image = tf.reshape(image, [height, width, 3])\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_for_eval(image, height, width, crop=True):\n",
    "    \"\"\"Preprocesses the given image for evaluation.\n",
    "    Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    crop: Whether or not to (center) crop the test images.\n",
    "    Returns:\n",
    "    A preprocessed image `Tensor`.\n",
    "    \"\"\"\n",
    "    if crop:\n",
    "        image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)\n",
    "    image = tf.reshape(image, [height, width, 3])\n",
    "    # image = tf.clip_by_value(image, 0., 1.)\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_image(image, height, width, is_training=False,\n",
    "                     color_distort=True, test_crop=True):\n",
    "    \"\"\"Preprocesses the given image.\n",
    "    Args:\n",
    "    image: `Tensor` representing an image of arbitrary size.\n",
    "    height: Height of output image.\n",
    "    width: Width of output image.\n",
    "    is_training: `bool` for whether the preprocessing is for training.\n",
    "    color_distort: whether to apply the color distortion.\n",
    "    test_crop: whether or not to extract a central crop of the images\n",
    "        (as for standard ImageNet evaluation) during the evaluation.\n",
    "    Returns:\n",
    "    A preprocessed image `Tensor` of range [0, 1].\n",
    "    \"\"\"\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    if is_training:\n",
    "        return preprocess_for_train(image, height, width, color_distort)\n",
    "    else:\n",
    "        return preprocess_for_eval(image, height, width, test_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mohit\\Anaconda3\\envs\\retina_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mohit\\Anaconda3\\envs\\retina_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "NCLASS   = 4\n",
    "IMG_SIZE = 224\n",
    "\n",
    "def resize_image(img, lb):\n",
    "  return tf.image.resize(img, (224,224)), tf.one_hot(lb, NCLASS)\n",
    "\n",
    "def augment_image(img, lb):\n",
    "  img, lb = resize_image(img, lb)\n",
    "  return preprocess_for_train(img, height=IMG_SIZE, width=IMG_SIZE), lb\n",
    "\n",
    "ds_train_augment = ds_train.map(augment_image)\n",
    "ds_val = ds_val.map(resize_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Weight Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25775.,  8434., 11112., 36498.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels = []\n",
    "labels = ds_train_augment.map(lambda x, y: y)\n",
    "for l in labels.batch(64).as_numpy_iterator():\n",
    "  y_labels.append(l)\n",
    "y_labels = np.vstack(y_labels)\n",
    "y_labels.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.7935887487875849, 1: 2.4252727057149635, 2: 1.8407802375809936, 3: 0.5604348183462108}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mohit\\Anaconda3\\envs\\retina_env\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass classes=[0, 1, 2, 3], y=[3 3 3 ... 3 0 3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "class_weights = compute_class_weight('balanced', [0, 1, 2, 3], y_labels.argmax(axis=1))\n",
    "class_weights = {i: w for i, w in enumerate(class_weights)}\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Classifier with ResNet Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basemodel(base_model='resnet', \n",
    "                     input_shape=(224,224,3),\n",
    "                     output_units=4):\n",
    "  \n",
    "    if base_model == 'resnet':\n",
    "        preprocess = tf.keras.applications.resnet_v2.preprocess_input\n",
    "        base_model = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet')\n",
    "    elif base_model == 'xception':\n",
    "        preprocess = tf.keras.applications.xception.preprocess_input\n",
    "        base_model = tf.keras.applications.Xception(include_top=False, weights='imagenet')\n",
    "    elif base_model == 'inception':\n",
    "        preprocess = tf.keras.layers.Lambda(lambda x: x)\n",
    "        base_model = tf.kaers.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet')\n",
    "    else:\n",
    "        raise f\"{base_model} not supported, choose from ['resnet', 'xception', 'inception']\"\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    pool    = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "    softmax   = tf.keras.layers.Dense(output_units, activation='softmax')\n",
    "\n",
    "    x = inputs\n",
    "    x = preprocess(x)\n",
    "    x = base_model(x)\n",
    "    x = tf.keras.layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "    x = pool(x)\n",
    "    x = flatten(x)\n",
    "    out = softmax(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_base = create_basemodel('resnet')\n",
    "\n",
    "metrics = ['accuracy']\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss', ),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath='resnet_model.{epoch:02d}-{val_loss:.2f}.h5'),]\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "resnet_base.compile(optimizer=optimizer, \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2557/2557 [==============================] - 162s 63ms/step - loss: 0.7330 - accuracy: 0.7402 - val_loss: 0.4418 - val_accuracy: 0.8431\n",
      "Epoch 2/20\n",
      "2557/2557 [==============================] - 162s 63ms/step - loss: 0.6188 - accuracy: 0.7797 - val_loss: 0.5046 - val_accuracy: 0.8162\n",
      "Epoch 3/20\n",
      "2557/2557 [==============================] - 163s 64ms/step - loss: 0.5961 - accuracy: 0.7870 - val_loss: 0.4796 - val_accuracy: 0.8275\n",
      "Epoch 4/20\n",
      "2557/2557 [==============================] - 163s 64ms/step - loss: 0.5745 - accuracy: 0.7928 - val_loss: 0.4796 - val_accuracy: 0.8275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21b72f74d08>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_base.fit(ds_train_augment.batch(32),\n",
    "                validation_data = ds_val.batch(32),\n",
    "                callbacks = callbacks,\n",
    "                class_weight = class_weights, \n",
    "                epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_resnet_base = create_basemodel('resnet')\n",
    "refine_resnet_base.load_weights('resnet_model.01-0.44.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 3s 57ms/step - loss: 0.4418 - accuracy: 0.8431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4418461322784424, 0.8431137800216675]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['accuracy']\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "refine_resnet_base.compile(optimizer=optimizer, \n",
    "                           loss='categorical_crossentropy', \n",
    "                           metrics=metrics)\n",
    "\n",
    "refine_resnet_base.evaluate(ds_val.batch(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_RealDiv_1 (Tenso [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Sub_1 (TensorFlo [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50v2 (Functional)      (None, None, None, 2048)  23564800  \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 5, 64)          1179712   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 24,744,772\n",
      "Trainable params: 1,179,972\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "refine_resnet_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the base model\n",
    "base_model = refine_resnet_base.layers[3]\n",
    "\n",
    "# unfreeze it\n",
    "base_model.trainable = True\n",
    "\n",
    "# select only the last resnet block for retraining\n",
    "# keeping the batchnorm layer unchanged\n",
    "for l in base_model.layers:\n",
    "  name = l.name\n",
    "  if name.startswith('conv5_block3') and not isinstance(l, tf.keras.layers.BatchNormalization):\n",
    "    l.trainable = True\n",
    "  else:\n",
    "    l.trainable = False\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "refine_resnet_base.compile(optimizer=optimizer, \n",
    "                           loss='categorical_crossentropy', \n",
    "                           metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2557/2557 [==============================] - 182s 71ms/step - loss: 0.5576 - accuracy: 0.8046 - val_loss: 0.3581 - val_accuracy: 0.8772\n",
      "Epoch 2/20\n",
      "2557/2557 [==============================] - 182s 71ms/step - loss: 0.4936 - accuracy: 0.8287 - val_loss: 0.3592 - val_accuracy: 0.8713\n",
      "Epoch 3/20\n",
      "2557/2557 [==============================] - 183s 72ms/step - loss: 0.4650 - accuracy: 0.8368 - val_loss: 0.3329 - val_accuracy: 0.8868\n",
      "Epoch 4/20\n",
      "2557/2557 [==============================] - 183s 72ms/step - loss: 0.4449 - accuracy: 0.8447 - val_loss: 0.2759 - val_accuracy: 0.9036\n",
      "Epoch 5/20\n",
      "2557/2557 [==============================] - 183s 72ms/step - loss: 0.4362 - accuracy: 0.8462 - val_loss: 0.3024 - val_accuracy: 0.8880\n",
      "Epoch 6/20\n",
      "2557/2557 [==============================] - 183s 71ms/step - loss: 0.4257 - accuracy: 0.8506 - val_loss: 0.3168 - val_accuracy: 0.8880\n",
      "Epoch 7/20\n",
      "2557/2557 [==============================] - 183s 71ms/step - loss: 0.4151 - accuracy: 0.8549 - val_loss: 0.2653 - val_accuracy: 0.9084\n",
      "Epoch 8/20\n",
      "2557/2557 [==============================] - 183s 71ms/step - loss: 0.4053 - accuracy: 0.8571 - val_loss: 0.2524 - val_accuracy: 0.9072\n",
      "Epoch 9/20\n",
      "2557/2557 [==============================] - 183s 71ms/step - loss: 0.4040 - accuracy: 0.8566 - val_loss: 0.2743 - val_accuracy: 0.9042\n",
      "Epoch 10/20\n",
      "2557/2557 [==============================] - 183s 72ms/step - loss: 0.3952 - accuracy: 0.8612 - val_loss: 0.2942 - val_accuracy: 0.8988\n",
      "Epoch 11/20\n",
      "2557/2557 [==============================] - 183s 72ms/step - loss: 0.3994 - accuracy: 0.8584 - val_loss: 0.2632 - val_accuracy: 0.9096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21b137f4a48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss', ),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath='finetune_resnet_model.{epoch:02d}-{val_loss:.2f}.h5'),]\n",
    "\n",
    "refine_resnet_base.fit(ds_train_augment.batch(32), \n",
    "                       validation_data = ds_val.batch(32),\n",
    "                       callbacks = callbacks,\n",
    "                       class_weight = class_weights, \n",
    "                       epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_resnet_base.load_weights('finetune_resnet_model.08-0.25.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 3s 91ms/step - loss: 0.0358 - accuracy: 0.9928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03580612689256668, 0.9927685856819153]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_resnet_base.evaluate(ds_test.map(resize_image).batch(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 4s 80ms/step - loss: 0.2524 - accuracy: 0.9072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2524038255214691, 0.9071856141090393]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_resnet_base.evaluate(ds_val.batch(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.models.base import WEIGHTS_DIRNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_path = str(WEIGHTS_DIRNAME)+\"/resnet_with_conv.h5\"\n",
    "wt_path = wt_path.replace('\\\\','/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_resnet_base.save_weights(wt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.networks.resnet_with_conv_finetune import resnetconvfinetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = resnetconvfinetune(input_shape = (224, 224, 3), output_shape = (4,))\n",
    "test_model.load_weights(wt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 3s 82ms/step - loss: 0.0358 - accuracy: 0.9928 0s - loss: 0.0382 - ac\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03580612689256668, 0.9927685856819153]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "test_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=metrics)\n",
    "test_model.evaluate(ds_test.map(resize_image).batch(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retinal_oct",
   "language": "python",
   "name": "retinal_oct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
